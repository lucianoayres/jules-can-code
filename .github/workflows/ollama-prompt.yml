name: Ollama Prompt via API

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: 'The prompt to send to Ollama'
        required: true
        type: string
      model:
        description: 'The Ollama model to use'
        required: false
        default: 'llama3.2'
        type: string
      format:
        description: 'The format for the response (e.g., json)'
        required: false
        default: 'json'
        type: string
      options:
        description: 'JSON string for additional Ollama options (e.g., {"temperature": 0.8})'
        required: false
        default: '{}'
        type: string
      stream:
        description: 'Whether to stream the response'
        required: false
        default: false
        type: boolean
      raw:
        description: 'Whether to use raw mode'
        required: false
        default: false
        type: boolean
      keep_alive:
        description: 'How long to keep the model loaded in memory (e.g., 5m, -1 for indefinite)'
        required: false
        default: '5m'
        type: string
jobs:
  run_ollama_prompt:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Ollama Binary Cache
        id: cache-ollama-binary
        uses: actions/cache@v4
        with:
          path: ~/.ollama_bin
          key: ollama-binary-${{ runner.os }}
          restore-keys: |
            ollama-binary-${{ runner.os }}

      - name: Install Ollama
        run: |
          if [ ! -f $HOME/.ollama_bin/ollama ]; then
            echo "Ollama binary not found in cache. Installing..."
            curl -fsSL https://ollama.com/install.sh | sh
            OLLAMA_INSTALL_PATH=$(which ollama)
            if [ -z "$OLLAMA_INSTALL_PATH" ]; then
              echo "::error::Ollama installation failed or binary not found in expected PATH."
              exit 1
            fi
            echo "Ollama installed at $OLLAMA_INSTALL_PATH"
            mkdir -p $HOME/.ollama_bin
            sudo mv $OLLAMA_INSTALL_PATH $HOME/.ollama_bin/ollama
            echo "Ollama binary moved to $HOME/.ollama_bin/ollama"
            # Ensure correct ownership by the runner user
            sudo chown runner:runner $HOME/.ollama_bin/ollama
            echo "Ownership of $HOME/.ollama_bin/ollama set to runner:runner"
          else
            echo "Ollama binary restored from cache."
            # It's possible the cache doesn't preserve ownership/permissions as expected by runner
            # If $HOME/.ollama_bin/ollama exists and is owned by root, chown it.
            if [ "$(stat -c '%U' $HOME/.ollama_bin/ollama)" = "root" ]; then
                sudo chown runner:runner $HOME/.ollama_bin/ollama
                echo "Ownership of cached $HOME/.ollama_bin/ollama changed to runner:runner"
            fi
          fi

          # Ensure the binary has execute permissions
          if [ -f $HOME/.ollama_bin/ollama ]; then
            # Ensure runner has execute permission.
            # chmod +x might not be enough if runner doesn't own the file and only owner has x perm.
            # However, chown above should handle ownership.
            # Using sudo chmod +x to be absolutely sure if there are any lingering ownership issues.
            # Or, more precisely, ensure user can execute: chmod u+x
            chmod u+x $HOME/.ollama_bin/ollama
            echo "Execute permissions (u+x) set on $HOME/.ollama_bin/ollama for the owner."
          else
            echo "::error::Ollama binary not found at $HOME/.ollama_bin/ollama to set permissions."
            exit 1
          fi

          echo "$HOME/.ollama_bin" >> $GITHUB_PATH
          echo "Listing permissions of Ollama binary before version check:"
          ls -la $HOME/.ollama_bin/ollama
          $HOME/.ollama_bin/ollama --version

      - name: Set up Ollama Models Cache
        id: cache-ollama-models
        uses: actions/cache@v4
        with:
          path: ~/.ollama/models
          key: ollama-models-${{ runner.os }}-${{ inputs.model }}
          restore-keys: |
            ollama-models-${{ runner.os }}-
      - name: Start Ollama Service
        run: |
          echo "Preparing to start Ollama Service. Checking binary permissions:"
          ls -la $HOME/.ollama_bin/ollama
          echo "Ensuring execute permissions again just in case..."
          chmod u+x $HOME/.ollama_bin/ollama
          
          $HOME/.ollama_bin/ollama serve &
          echo "Waiting for Ollama service to start..."
          # Timeout logic remains the same
          timeout_seconds=60
          start_time=$(date +%s)
          while ! curl -s -o /dev/null http://localhost:11434; do
            current_time=$(date +%s)
            elapsed_time=$((current_time - start_time))
            if [ "$elapsed_time" -ge "$timeout_seconds" ]; then
              echo "::error::Timeout waiting for Ollama service to start."
              # Attempt to capture Ollama service logs if it failed to start
              if pgrep -x "ollama" > /dev/null; then
                echo "Ollama process is running. Logs (if any via journalctl, may need sudo or be unavailable):"
                # sudo journalctl -u ollama --no-pager -n 50 # This might not work due to permissions or how ollama logs without systemd service properly configured by script
              else
                echo "Ollama process is not running."
              fi
              # Check for any ollama log files if they exist in a known location (speculative)
              if [ -d "$HOME/.ollama/logs" ]; then
                 echo "Checking for logs in $HOME/.ollama/logs:"
                 ls -la $HOME/.ollama/logs
                 cat $HOME/.ollama/logs/* 2>/dev/null || echo "No logs found or accessible in $HOME/.ollama/logs"
              fi
              exit 1
            fi
            sleep 1
          done
          echo "Ollama service is up and running."

      - name: Pull Ollama Model
        run: |
          $HOME/.ollama_bin/ollama pull ${{ inputs.model }} # Ensure this line is updated
          $HOME/.ollama_bin/ollama list # Ensure this line is updated

      - name: Construct API Payload
        id: construct_payload
        run: |
          echo "Constructing API payload..."
          # Safely parse the options JSON, ensuring it's valid
          if [ -z "${{ inputs.options }}" ]; then
            echo "::debug::'options' input is empty. Defaulting to empty JSON object."
            options_json="{}"
          elif ! echo "${{ inputs.options }}" | jq -e empty > /dev/null 2>&1; then
            echo "::warning::Invalid JSON provided for 'options'. Defaulting to empty JSON object. Input was: ${{ inputs.options }}"
            options_json="{}"
          else
            echo "::debug::'options' input is valid JSON."
            options_json='${{ inputs.options }}'
          fi

          # Construct the payload using jq for robustness
          payload=$(jq -n \
            --arg model "${{ inputs.model }}" \
            --arg prompt "${{ inputs.prompt }}" \
            --arg format "${{ inputs.format }}" \
            --argjson stream ${{ inputs.stream }} \
            --argjson raw ${{ inputs.raw }} \
            --arg keep_alive "${{ inputs.keep_alive }}" \
            --argjson options_override "$options_json" \
            '{model: $model, prompt: $prompt, format: $format, stream: $stream, raw: $raw, keep_alive: $keep_alive} + $options_override')

          echo "Payload:"
          echo "$payload"
          # Make payload available to subsequent steps
          echo "api_payload<<EOF" >> $GITHUB_OUTPUT
          echo "$payload" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        shell: bash

      - name: Call Ollama API
        id: call_api
        run: |
          echo "Sending request to Ollama API..."
          response_json=$(curl -s -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d '${{ steps.construct_payload.outputs.api_payload }}')

          echo "API Response:"
          echo "$response_json"

          # Make response available for the next step
          echo "api_response<<EOF" >> $GITHUB_OUTPUT
          echo "$response_json" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        shell: bash

      - name: Display LLM Response
        run: |
          echo "LLM Response (raw):"
          echo "${{ steps.call_api.outputs.api_response }}"

          # If response is JSON and not streamed, try to pretty print
          if [ "${{ inputs.format }}" == "json" ] && [ "${{ inputs.stream }}" == "false" ]; then
            echo "LLM Response (pretty-printed JSON):"
            echo "${{ steps.call_api.outputs.api_response }}" | jq .
          fi
        shell: bash
